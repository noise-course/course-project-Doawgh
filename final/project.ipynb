{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86539a33",
   "metadata": {},
   "source": [
    "OS detection Justin Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f69acf7",
   "metadata": {},
   "source": [
    "Having generated the nprints, and the metadata which is written in file_name,label, we now prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7653f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "split_dir = os.path.expanduser(\"/workspaces/course-project-Doawgh/final/split_output\")\n",
    "nprint_dir = os.path.expanduser(\"/workspaces/course-project-Doawgh/final/nprints\")\n",
    "metadata_file = os.path.join(split_dir, \"metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb37eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "# load metadata\n",
    "metadata = pd.read_csv(metadata_file)\n",
    "metadata.set_index(\"File\", inplace=True)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "nprint_files = os.listdir(nprint_dir)\n",
    "count = 0\n",
    "for filename in nprint_files:\n",
    "\n",
    "    # only process .nprint files\n",
    "    if not filename.endswith(\".nprint\"):\n",
    "        continue\n",
    "    \n",
    "    nprint_path = os.path.join(nprint_dir, filename)\n",
    "\n",
    "    # load nPrint CSV\n",
    "    nprint_data = pd.read_csv(nprint_path)\n",
    "\n",
    "    # keep only numeric columns and flatten\n",
    "    flat = nprint_data.select_dtypes(include=[np.number]).values.flatten()\n",
    "    X.append(flat)\n",
    "\n",
    "    # map filename to label\n",
    "    key = filename.replace(\".nprint\", \".pcap\")\n",
    "    if key in metadata.index:\n",
    "        label = metadata.loc[key, \"Label\"]\n",
    "        y.append(label)\n",
    "    else:\n",
    "        raise ValueError(f\"Label not found for {key}\")\n",
    "    \n",
    "    # added for the sake of time so that training doesn't take hours/days\n",
    "    # if you want to increase quality of prediction then raise this number\n",
    "    count += 1\n",
    "    if count >= 200:\n",
    "        break\n",
    "\n",
    "# convert lists to df for AutoGluon\n",
    "X_df = pd.DataFrame(X)\n",
    "X_df['Label'] = y\n",
    "\n",
    "X_features = X_df.drop(columns='Label')\n",
    "y_labels = X_df['Label']\n",
    "X_features_filled = X_features.fillna(X_features.mean())\n",
    "# reduces the number of features, meaning the training takes less time to run. Up to around 96k for n_components\n",
    "svd = TruncatedSVD(n_components=2000, random_state=42)\n",
    "X_reduced = svd.fit_transform(X_features_filled)\n",
    "\n",
    "\n",
    "X_reduced_df = pd.DataFrame(X_reduced)\n",
    "X_reduced_df['Label'] = y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14658da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20251212_123846\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.3\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025\n",
      "CPU Count:          16\n",
      "Memory Avail:       3.58 GB / 7.35 GB (48.7%)\n",
      "Disk Space Avail:   933.16 GB / 1006.85 GB (92.7%)\n",
      "===================================================\n",
      "Presets specified: ['good_quality']\n",
      "Using hyperparameters preset: hyperparameters='light'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "\t\tContext path: \"/home/doawgh/nprint/AutogluonModels/ag-20251212_123846/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Beginning AutoGluon training ... Time limit = 150s\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m AutoGluon will save models to \"/home/doawgh/nprint/AutogluonModels/ag-20251212_123846/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Train Data Rows:    142\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Train Data Columns: 200\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Label Column:       Label\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Problem Type:       multiclass\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Warning: Updated label_count_threshold from 10 to 6 to avoid cutting too many classes.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Train Data Class Count: 13\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tAvailable Memory:                    3856.45 MB\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tTrain Data (Original)  Memory Usage: 0.22 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t\t('float', []) : 200 | ['0', '1', '2', '3', '4', ...]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t\t('float', []) : 200 | ['0', '1', '2', '3', '4', ...]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.2s = Fit runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t200 features in original data used to generate 200 features in processed data.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tTrain Data (Processed) Memory Usage: 0.22 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Data preprocessing and feature engineering runtime = 0.23s ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'balanced_accuracy'\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t'NN_TORCH': [{}],\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t'CAT': [{}],\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t'XGB': [{}],\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t'FASTAI': [{}],\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 99.82s of the 149.77s of remaining time.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.06%)\n",
      "\u001b[36m(_ray_fit pid=5799)\u001b[0m Metric balanced_accuracy is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=5799)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.1294\t = Validation score   (balanced_accuracy)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t7.74s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.09s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 88.81s of the 138.76s of remaining time.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=9.65%)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.3367\t = Validation score   (balanced_accuracy)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t3.35s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.02s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 81.15s of the 131.09s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=5797)\u001b[0m Metric balanced_accuracy is not supported by this model - using log_loss instead\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5800)\u001b[0m No improvement since epoch 6: early stopping\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.33%)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.3407\t = Validation score   (balanced_accuracy)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t3.68s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.02s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 74.24s of the 124.19s of remaining time.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.2574\t = Validation score   (balanced_accuracy)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t3.29s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.11s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 70.73s of the 120.68s of remaining time.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.2343\t = Validation score   (balanced_accuracy)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t3.05s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 67.47s of the 117.42s of remaining time.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 14.04% memory usage per fold, 56.16%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=14.04%)\n",
      "\u001b[36m(_ray_fit pid=7342)\u001b[0m \tRan out of time, early stopping on iteration 222.\n",
      "\u001b[36m(_ray_fit pid=7777)\u001b[0m \tRan out of time, early stopping on iteration 245.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.2775\t = Validation score   (balanced_accuracy)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t56.49s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 8.36s of the 58.31s of remaining time.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.2613\t = Validation score   (balanced_accuracy)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t3.18s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 4.96s of the 54.91s of remaining time.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.2536\t = Validation score   (balanced_accuracy)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t3.08s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 1.63s of the 51.58s of remaining time.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.91%)\n",
      "\u001b[36m(_ray_fit pid=7779)\u001b[0m \tRan out of time, early stopping on iteration 240.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=8303, ip=192.168.69.68)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return func(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1683, in fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self._Booster = train(\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                     ^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return func(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/callback.py\", line 264, in after_iteration\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 2352, in eval_set\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     _check_call(\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m xgboost.core.XGBoostError: [06:40:32] /workspace/src/common/common.cu:16: /workspace/src/common/cuda_dr_utils.cc: 30: cudaErrorNoDevice: no CUDA-capable device is detected\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Stack trace:\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (0) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6e7c) [0x771204ca6e7c]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (1) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5b61e) [0x77120545b61e]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (2) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x38303b) [0x771204d8303b]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (3) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x3840ac) [0x771204d840ac]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (4) /lib/x86_64-linux-gnu/libc.so.6(+0xa1ed3) [0x77131b0a1ed3]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (5) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x382f68) [0x771204d82f68]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (6) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5cb55) [0x77120545cb55]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (7) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5bacd) [0x77120545bacd]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (8) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xd940b6) [0x7712057940b6]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2171, in _train_and_save\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 389, in _fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self._fit_folds(\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 868, in _fit_folds\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 723, in after_all_folds_scheduled\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 664, in _run_parallel\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 620, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     raise processed_exception\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 583, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                                                                                                                                      ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     raise value.as_instanceof_cause()\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m ray.exceptions.RayTaskError(XGBoostError): \u001b[36mray::_ray_fit()\u001b[39m (pid=8303, ip=192.168.69.68)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return func(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1683, in fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self._Booster = train(\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                     ^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return func(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/callback.py\", line 264, in after_iteration\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 2352, in eval_set\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     _check_call(\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m xgboost.core.XGBoostError: [06:40:32] /workspace/src/common/common.cu:16: /workspace/src/common/cuda_dr_utils.cc: 30: cudaErrorNoDevice: no CUDA-capable device is detected\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Stack trace:\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (0) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6e7c) [0x771204ca6e7c]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (1) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5b61e) [0x77120545b61e]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (2) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x38303b) [0x771204d8303b]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (3) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x3840ac) [0x771204d840ac]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (4) /lib/x86_64-linux-gnu/libc.so.6(+0xa1ed3) [0x77131b0a1ed3]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (5) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x382f68) [0x771204d82f68]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (6) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5cb55) [0x77120545cb55]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (7) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5bacd) [0x77120545bacd]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (8) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xd940b6) [0x7712057940b6]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 149.77s of the 47.63s of remaining time.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.3407\t = Validation score   (balanced_accuracy)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.12s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting 11 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 47.48s of the 47.47s of remaining time.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.11%)\n",
      "\u001b[36m(_ray_fit pid=8798)\u001b[0m Metric balanced_accuracy is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_ray_fit pid=8798)\u001b[0m No improvement since epoch 0: early stopping\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.2194\t = Validation score   (balanced_accuracy)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t7.44s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.11s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 36.56s of the 36.56s of remaining time.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.74% memory usage per fold, 42.96%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=10.74%)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.3301\t = Validation score   (balanced_accuracy)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t9.65s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 23.56s of the 23.55s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=8800)\u001b[0m Metric balanced_accuracy is not supported by this model - using log_loss instead\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8800)\u001b[0m No improvement since epoch 6: early stopping\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.67%)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.3614\t = Validation score   (balanced_accuracy)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t4.78s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 15.52s of the 15.51s of remaining time.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.2645\t = Validation score   (balanced_accuracy)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t3.25s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 12.03s of the 12.02s of remaining time.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.2445\t = Validation score   (balanced_accuracy)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t3.1s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 8.71s of the 8.70s of remaining time.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.63% memory usage per fold, 66.54%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=16.63%)\n",
      "\u001b[36m(_ray_fit pid=10393)\u001b[0m \tRan out of time, early stopping on iteration 18.\n",
      "\u001b[36m(_ray_fit pid=10719)\u001b[0m \tRan out of time, early stopping on iteration 18.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.1205\t = Validation score   (balanced_accuracy)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t9.25s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.11s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 149.77s of the -2.58s of remaining time.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L2': 1.0}\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.3614\t = Validation score   (balanced_accuracy)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.1s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m AutoGluon training complete, total runtime = 152.72s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 88.5 rows/s (18 batch size)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=8306, ip=192.168.69.68)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return func(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1683, in fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self._Booster = train(\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                     ^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return func(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/callback.py\", line 264, in after_iteration\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 2352, in eval_set\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     _check_call(\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m xgboost.core.XGBoostError: [06:40:32] /workspace/src/common/common.cu:16: /workspace/src/common/cuda_dr_utils.cc: 30: cudaErrorNoDevice: no CUDA-capable device is detected\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Stack trace:\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (0) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6e7c) [0x7d4c7c4a6e7c]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (1) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5b61e) [0x7d4c7cc5b61e]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (2) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x38303b) [0x7d4c7c58303b]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (3) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x3840ac) [0x7d4c7c5840ac]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (4) /lib/x86_64-linux-gnu/libc.so.6(+0xa1ed3) [0x7d4d928a1ed3]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (5) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x382f68) [0x7d4c7c582f68]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (6) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5cb55) [0x7d4c7cc5cb55]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (7) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5bacd) [0x7d4c7cc5bacd]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (8) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xd940b6) [0x7d4c7cf940b6]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=8305, ip=192.168.69.68)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return func(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1683, in fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self._Booster = train(\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                     ^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return func(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/callback.py\", line 264, in after_iteration\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 2352, in eval_set\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     _check_call(\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m xgboost.core.XGBoostError: [06:40:32] /workspace/src/common/common.cu:16: /workspace/src/common/cuda_dr_utils.cc: 30: cudaErrorNoDevice: no CUDA-capable device is detected\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Stack trace:\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (0) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6e7c) [0x70dbea2a6e7c]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (1) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5b61e) [0x70dbeaa5b61e]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (2) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x38303b) [0x70dbea38303b]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (3) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x3840ac) [0x70dbea3840ac]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (4) /lib/x86_64-linux-gnu/libc.so.6(+0xa1ed3) [0x70dd004a1ed3]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (5) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x382f68) [0x70dbea382f68]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (6) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5cb55) [0x70dbeaa5cb55]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (7) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5bacd) [0x70dbeaa5bacd]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (8) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xd940b6) [0x70dbead940b6]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=8307, ip=192.168.69.68)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return func(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1683, in fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self._Booster = train(\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                     ^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return func(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/callback.py\", line 264, in after_iteration\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 2352, in eval_set\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     _check_call(\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m xgboost.core.XGBoostError: [06:40:32] /workspace/src/common/common.cu:16: /workspace/src/common/cuda_dr_utils.cc: 30: cudaErrorNoDevice: no CUDA-capable device is detected\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Stack trace:\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (0) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6e7c) [0x7750e9ea6e7c]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (1) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5b61e) [0x7750ea65b61e]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (2) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x38303b) [0x7750e9f8303b]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (3) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x3840ac) [0x7750e9f840ac]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (4) /lib/x86_64-linux-gnu/libc.so.6(+0xa1ed3) [0x7752000a1ed3]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (5) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x382f68) [0x7750e9f82f68]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (6) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5cb55) [0x7750ea65cb55]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (7) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5bacd) [0x7750ea65bacd]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (8) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xd940b6) [0x7750ea9940b6]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=8304, ip=192.168.69.68)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return func(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1683, in fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self._Booster = train(\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                     ^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return func(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/callback.py\", line 264, in after_iteration\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 2352, in eval_set\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     _check_call(\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m xgboost.core.XGBoostError: [06:40:32] /workspace/src/common/common.cu:16: /workspace/src/common/cuda_dr_utils.cc: 30: cudaErrorNoDevice: no CUDA-capable device is detected\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Stack trace:\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (0) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6e7c) [0x78f9b56a6e7c]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (1) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5b61e) [0x78f9b5e5b61e]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (2) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x38303b) [0x78f9b578303b]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (3) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x3840ac) [0x78f9b57840ac]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (4) /lib/x86_64-linux-gnu/libc.so.6(+0xa1ed3) [0x78facbaa1ed3]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (5) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x382f68) [0x78f9b5782f68]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (6) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5cb55) [0x78f9b5e5cb55]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (7) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5bacd) [0x78f9b5e5bacd]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (8) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xd940b6) [0x78f9b61940b6]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=8300, ip=192.168.69.68)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return func(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1683, in fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self._Booster = train(\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                     ^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return func(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/callback.py\", line 264, in after_iteration\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 2352, in eval_set\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     _check_call(\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m xgboost.core.XGBoostError: [06:40:32] /workspace/src/common/common.cu:16: /workspace/src/common/cuda_dr_utils.cc: 30: cudaErrorNoDevice: no CUDA-capable device is detected\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Stack trace:\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (0) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6e7c) [0x713d304a6e7c]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (1) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5b61e) [0x713d30c5b61e]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (2) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x38303b) [0x713d3058303b]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (3) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x3840ac) [0x713d305840ac]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (4) /lib/x86_64-linux-gnu/libc.so.6(+0xa1ed3) [0x713e466a1ed3]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (5) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x382f68) [0x713d30582f68]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (6) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5cb55) [0x713d30c5cb55]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (7) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5bacd) [0x713d30c5bacd]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (8) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xd940b6) [0x713d30f940b6]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=8302, ip=192.168.69.68)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return func(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1683, in fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self._Booster = train(\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                     ^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return func(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/callback.py\", line 264, in after_iteration\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 2352, in eval_set\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     _check_call(\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m xgboost.core.XGBoostError: [06:40:32] /workspace/src/common/common.cu:16: /workspace/src/common/cuda_dr_utils.cc: 30: cudaErrorNoDevice: no CUDA-capable device is detected\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Stack trace:\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (0) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6e7c) [0x76e8ea6a6e7c]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (1) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5b61e) [0x76e8eae5b61e]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (2) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x38303b) [0x76e8ea78303b]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (3) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x3840ac) [0x76e8ea7840ac]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (4) /lib/x86_64-linux-gnu/libc.so.6(+0xa1ed3) [0x76ea008a1ed3]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (5) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x382f68) [0x76e8ea782f68]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (6) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5cb55) [0x76e8eae5cb55]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (7) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5bacd) [0x76e8eae5bacd]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (8) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xd940b6) [0x76e8eb1940b6]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=8301, ip=192.168.69.68)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return func(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1683, in fit\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     self._Booster = train(\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                     ^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     return func(**kwargs)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m            ^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/callback.py\", line 264, in after_iteration\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 2352, in eval_set\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     _check_call(\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m     raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m xgboost.core.XGBoostError: [06:40:32] /workspace/src/common/common.cu:16: /workspace/src/common/cuda_dr_utils.cc: 30: cudaErrorNoDevice: no CUDA-capable device is detected\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Stack trace:\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (0) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6e7c) [0x72e32bea6e7c]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (1) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5b61e) [0x72e32c65b61e]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (2) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x38303b) [0x72e32bf8303b]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (3) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x3840ac) [0x72e32bf840ac]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (4) /lib/x86_64-linux-gnu/libc.so.6(+0xa1ed3) [0x72e4422a1ed3]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (5) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x382f68) [0x72e32bf82f68]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (6) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5cb55) [0x72e32c65cb55]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (7) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5bacd) [0x72e32c65bacd]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m   [bt] (8) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xd940b6) [0x72e32c9940b6]\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Metric balanced_accuracy is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tStopping at the best epoch learned earlier - 1.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.34s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t1.45s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\u001b[36m(_ray_fit pid=10764)\u001b[0m \tRan out of time, early stopping on iteration 19.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.74s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t3.29s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.11s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t3.05s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t9.61s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t3.18s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t3.08s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.12s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2_FULL ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Metric balanced_accuracy is not supported by this model - using log_loss instead\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tStopping at the best epoch learned earlier - 5.\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.44s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.7s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t1.28s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: RandomForestGini_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t3.25s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: RandomForestEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t3.1s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.8s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L2': 1.0}\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m \t0.1s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Updated best model to \"LightGBM_BAG_L2_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"LightGBM_BAG_L2_FULL\" for predict() and predict_proba().\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Refit complete, total runtime = 19.64s ... Best model: \"LightGBM_BAG_L2_FULL\"\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/doawgh/nprint/AutogluonModels/ag-20251212_123846/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=4986)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                           model  score_holdout  score_val        eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0           CatBoost_BAG_L1_FULL       0.423077   0.277465  balanced_accuracy        0.011944            NaN   9.612221                 0.011944                     NaN           9.612221            1       True          6\n",
      "1     ExtraTreesGini_BAG_L1_FULL       0.346154   0.261257  balanced_accuracy        0.130860       0.117031   3.175951                 0.130860                0.117031           3.175951            1       True          7\n",
      "2         LightGBMXT_BAG_L1_FULL       0.269231   0.336743  balanced_accuracy        0.016513            NaN   1.446040                 0.016513                     NaN           1.446040            1       True          2\n",
      "3   RandomForestGini_BAG_L1_FULL       0.269231   0.257432  balanced_accuracy        0.142072       0.113069   3.294536                 0.142072                0.113069           3.294536            1       True          4\n",
      "4    NeuralNetFastAI_BAG_L2_FULL       0.230769   0.219365  balanced_accuracy        0.327304            NaN  17.606058                 0.019396                     NaN           0.439599            2       True         10\n",
      "5           LightGBM_BAG_L1_FULL       0.230769   0.340659  balanced_accuracy        0.005651            NaN   0.740706                 0.005651                     NaN           0.740706            1       True          3\n",
      "6       WeightedEnsemble_L2_FULL       0.230769   0.340659  balanced_accuracy        0.008558            NaN   0.856049                 0.002908                     NaN           0.115343            2       True          9\n",
      "7   RandomForestEntr_BAG_L1_FULL       0.230769   0.234312  balanced_accuracy        0.141802       0.120026   3.051049                 0.141802                0.120026           3.051049            1       True          5\n",
      "8           LightGBM_BAG_L2_FULL       0.230769   0.361369  balanced_accuracy        0.314965            NaN  18.446611                 0.007056                     NaN           1.280152            2       True         12\n",
      "9         LightGBMXT_BAG_L2_FULL       0.230769   0.330058  balanced_accuracy        0.315829            NaN  17.864387                 0.007921                     NaN           0.697927            2       True         11\n",
      "10      WeightedEnsemble_L3_FULL       0.230769   0.361369  balanced_accuracy        0.318497            NaN  18.543976                 0.003532                     NaN           0.097364            3       True         16\n",
      "11          CatBoost_BAG_L2_FULL       0.230769   0.120456  balanced_accuracy        0.321364            NaN  17.965313                 0.013456                     NaN           0.798854            2       True         15\n",
      "12  RandomForestGini_BAG_L2_FULL       0.230769   0.264483  balanced_accuracy        0.444470            NaN  20.413619                 0.136562                0.124872           3.247159            2       True         13\n",
      "13  RandomForestEntr_BAG_L2_FULL       0.230769   0.244513  balanced_accuracy        0.453211            NaN  20.261721                 0.145303                0.120219           3.095261            2       True         14\n",
      "14    ExtraTreesEntr_BAG_L1_FULL       0.192308   0.253649  balanced_accuracy        0.152229       0.121862   3.078134                 0.152229                0.121862           3.078134            1       True          8\n",
      "15   NeuralNetFastAI_BAG_L1_FULL       0.153846   0.129431  balanced_accuracy        0.017381            NaN   0.343046                 0.017381                     NaN           0.343046            1       True          1\n",
      "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
      "\t178s\t = DyStack   runtime |\t422s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=0.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
      "Beginning AutoGluon training ... Time limit = 422s\n",
      "AutoGluon will save models to \"/home/doawgh/nprint/AutogluonModels/ag-20251212_123846\"\n",
      "Train Data Rows:    160\n",
      "Train Data Columns: 200\n",
      "Label Column:       Label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Warning: Updated label_count_threshold from 10 to 7 to avoid cutting too many classes.\n",
      "Train Data Class Count: 13\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3592.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.24 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 200 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 200 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.2s = Fit runtime\n",
      "\t200 features in original data used to generate 200 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.24 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.28s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'balanced_accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 422.09s of the 422.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.08%)\n",
      "\t0.127\t = Validation score   (balanced_accuracy)\n",
      "\t7.19s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 412.09s of the 412.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=7.20%)\n",
      "\t0.3282\t = Validation score   (balanced_accuracy)\n",
      "\t4.7s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 403.04s of the 403.03s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.37%)\n",
      "\t0.3627\t = Validation score   (balanced_accuracy)\n",
      "\t5.04s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 394.47s of the 394.46s of remaining time.\n",
      "\t0.2622\t = Validation score   (balanced_accuracy)\n",
      "\t1.48s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 392.76s of the 392.75s of remaining time.\n",
      "\t0.2811\t = Validation score   (balanced_accuracy)\n",
      "\t1.04s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 391.51s of the 391.50s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 14.29% memory usage per fold, 57.15%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=4, gpus=0, memory=14.29%)\n",
      "\t0.3713\t = Validation score   (balanced_accuracy)\n",
      "\t315.34s\t = Training   runtime\n",
      "\t-0.68s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 74.91s of the 74.90s of remaining time.\n",
      "\t0.2986\t = Validation score   (balanced_accuracy)\n",
      "\t1.09s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 73.61s of the 73.60s of remaining time.\n",
      "\t0.2727\t = Validation score   (balanced_accuracy)\n",
      "\t1.12s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 72.23s of the 72.22s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=3.08%)\n",
      "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=15101, ip=192.168.69.68)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1683, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n",
      "    if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/callback.py\", line 264, in after_iteration\n",
      "    score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 2352, in eval_set\n",
      "    _check_call(\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [06:47:38] /workspace/src/common/common.cu:16: /workspace/src/common/cuda_dr_utils.cc: 30: cudaErrorNoDevice: no CUDA-capable device is detected\n",
      "Stack trace:\n",
      "  [bt] (0) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6e7c) [0x7e6472ca6e7c]\n",
      "  [bt] (1) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5b61e) [0x7e647345b61e]\n",
      "  [bt] (2) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x38303b) [0x7e6472d8303b]\n",
      "  [bt] (3) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x3840ac) [0x7e6472d840ac]\n",
      "  [bt] (4) /lib/x86_64-linux-gnu/libc.so.6(+0xa1ed3) [0x7e6588ea1ed3]\n",
      "  [bt] (5) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x382f68) [0x7e6472d82f68]\n",
      "  [bt] (6) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5cb55) [0x7e647345cb55]\n",
      "  [bt] (7) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5bacd) [0x7e647345bacd]\n",
      "  [bt] (8) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xd940b6) [0x7e64737940b6]\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2171, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 389, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 868, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 723, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 664, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 620, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 583, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(XGBoostError): \u001b[36mray::_ray_fit()\u001b[39m (pid=15101, ip=192.168.69.68)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1683, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n",
      "    if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/callback.py\", line 264, in after_iteration\n",
      "    score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 2352, in eval_set\n",
      "    _check_call(\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [06:47:38] /workspace/src/common/common.cu:16: /workspace/src/common/cuda_dr_utils.cc: 30: cudaErrorNoDevice: no CUDA-capable device is detected\n",
      "Stack trace:\n",
      "  [bt] (0) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6e7c) [0x7e6472ca6e7c]\n",
      "  [bt] (1) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5b61e) [0x7e647345b61e]\n",
      "  [bt] (2) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x38303b) [0x7e6472d8303b]\n",
      "  [bt] (3) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x3840ac) [0x7e6472d840ac]\n",
      "  [bt] (4) /lib/x86_64-linux-gnu/libc.so.6(+0xa1ed3) [0x7e6588ea1ed3]\n",
      "  [bt] (5) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x382f68) [0x7e6472d82f68]\n",
      "  [bt] (6) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5cb55) [0x7e647345cb55]\n",
      "  [bt] (7) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5bacd) [0x7e647345bacd]\n",
      "  [bt] (8) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xd940b6) [0x7e64737940b6]\n",
      "2025-12-12 06:47:44,639\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=15106, ip=192.168.69.68)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1683, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n",
      "    if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/callback.py\", line 264, in after_iteration\n",
      "    score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 2352, in eval_set\n",
      "    _check_call(\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [06:47:38] /workspace/src/common/common.cu:16: /workspace/src/common/cuda_dr_utils.cc: 30: cudaErrorNoDevice: no CUDA-capable device is detected\n",
      "Stack trace:\n",
      "  [bt] (0) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6e7c) [0x7b5ed80a6e7c]\n",
      "  [bt] (1) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5b61e) [0x7b5ed885b61e]\n",
      "  [bt] (2) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x38303b) [0x7b5ed818303b]\n",
      "  [bt] (3) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x3840ac) [0x7b5ed81840ac]\n",
      "  [bt] (4) /lib/x86_64-linux-gnu/libc.so.6(+0xa1ed3) [0x7b5fee4a1ed3]\n",
      "  [bt] (5) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x382f68) [0x7b5ed8182f68]\n",
      "  [bt] (6) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5cb55) [0x7b5ed885cb55]\n",
      "  [bt] (7) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5bacd) [0x7b5ed885bacd]\n",
      "  [bt] (8) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xd940b6) [0x7b5ed8b940b6]\n",
      "2025-12-12 06:47:44,644\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=15105, ip=192.168.69.68)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1683, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n",
      "    if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/callback.py\", line 264, in after_iteration\n",
      "    score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 2352, in eval_set\n",
      "    _check_call(\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [06:47:38] /workspace/src/common/common.cu:16: /workspace/src/common/cuda_dr_utils.cc: 30: cudaErrorNoDevice: no CUDA-capable device is detected\n",
      "Stack trace:\n",
      "  [bt] (0) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6e7c) [0x74d3f92a6e7c]\n",
      "  [bt] (1) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5b61e) [0x74d3f9a5b61e]\n",
      "  [bt] (2) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x38303b) [0x74d3f938303b]\n",
      "  [bt] (3) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x3840ac) [0x74d3f93840ac]\n",
      "  [bt] (4) /lib/x86_64-linux-gnu/libc.so.6(+0xa1ed3) [0x74d50f6a1ed3]\n",
      "  [bt] (5) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x382f68) [0x74d3f9382f68]\n",
      "  [bt] (6) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5cb55) [0x74d3f9a5cb55]\n",
      "  [bt] (7) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5bacd) [0x74d3f9a5bacd]\n",
      "  [bt] (8) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xd940b6) [0x74d3f9d940b6]\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 62.12s of the 62.11s of remaining time.\n",
      "2025-12-12 06:47:44,650\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=15108, ip=192.168.69.68)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1683, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n",
      "    if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/callback.py\", line 264, in after_iteration\n",
      "    score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 2352, in eval_set\n",
      "    _check_call(\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [06:47:38] /workspace/src/common/common.cu:16: /workspace/src/common/cuda_dr_utils.cc: 30: cudaErrorNoDevice: no CUDA-capable device is detected\n",
      "Stack trace:\n",
      "  [bt] (0) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6e7c) [0x7430418a6e7c]\n",
      "  [bt] (1) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5b61e) [0x74304205b61e]\n",
      "  [bt] (2) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x38303b) [0x74304198303b]\n",
      "  [bt] (3) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x3840ac) [0x7430419840ac]\n",
      "  [bt] (4) /lib/x86_64-linux-gnu/libc.so.6(+0xa1ed3) [0x743157ca1ed3]\n",
      "  [bt] (5) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x382f68) [0x743041982f68]\n",
      "  [bt] (6) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5cb55) [0x74304205cb55]\n",
      "  [bt] (7) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5bacd) [0x74304205bacd]\n",
      "  [bt] (8) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xd940b6) [0x7430423940b6]\n",
      "2025-12-12 06:47:44,669\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=15103, ip=192.168.69.68)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1683, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n",
      "    if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/callback.py\", line 264, in after_iteration\n",
      "    score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 2352, in eval_set\n",
      "    _check_call(\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [06:47:38] /workspace/src/common/common.cu:16: /workspace/src/common/cuda_dr_utils.cc: 30: cudaErrorNoDevice: no CUDA-capable device is detected\n",
      "Stack trace:\n",
      "  [bt] (0) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6e7c) [0x7d991baa6e7c]\n",
      "  [bt] (1) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5b61e) [0x7d991c25b61e]\n",
      "  [bt] (2) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x38303b) [0x7d991bb8303b]\n",
      "  [bt] (3) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x3840ac) [0x7d991bb840ac]\n",
      "  [bt] (4) /lib/x86_64-linux-gnu/libc.so.6(+0xa1ed3) [0x7d9a31ca1ed3]\n",
      "  [bt] (5) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x382f68) [0x7d991bb82f68]\n",
      "  [bt] (6) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5cb55) [0x7d991c25cb55]\n",
      "  [bt] (7) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5bacd) [0x7d991c25bacd]\n",
      "  [bt] (8) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xd940b6) [0x7d991c5940b6]\n",
      "2025-12-12 06:47:44,684\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=15102, ip=192.168.69.68)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1683, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n",
      "    if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/callback.py\", line 264, in after_iteration\n",
      "    score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 2352, in eval_set\n",
      "    _check_call(\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [06:47:38] /workspace/src/common/common.cu:16: /workspace/src/common/cuda_dr_utils.cc: 30: cudaErrorNoDevice: no CUDA-capable device is detected\n",
      "Stack trace:\n",
      "  [bt] (0) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6e7c) [0x7bd78fca6e7c]\n",
      "  [bt] (1) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5b61e) [0x7bd79045b61e]\n",
      "  [bt] (2) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x38303b) [0x7bd78fd8303b]\n",
      "  [bt] (3) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x3840ac) [0x7bd78fd840ac]\n",
      "  [bt] (4) /lib/x86_64-linux-gnu/libc.so.6(+0xa1ed3) [0x7bd8a5ea1ed3]\n",
      "  [bt] (5) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x382f68) [0x7bd78fd82f68]\n",
      "  [bt] (6) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5cb55) [0x7bd79045cb55]\n",
      "  [bt] (7) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5bacd) [0x7bd79045bacd]\n",
      "  [bt] (8) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xd940b6) [0x7bd7907940b6]\n",
      "2025-12-12 06:47:44,697\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=15104, ip=192.168.69.68)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1683, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n",
      "    if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/callback.py\", line 264, in after_iteration\n",
      "    score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 2352, in eval_set\n",
      "    _check_call(\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [06:47:38] /workspace/src/common/common.cu:16: /workspace/src/common/cuda_dr_utils.cc: 30: cudaErrorNoDevice: no CUDA-capable device is detected\n",
      "Stack trace:\n",
      "  [bt] (0) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6e7c) [0x79c410ea6e7c]\n",
      "  [bt] (1) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5b61e) [0x79c41165b61e]\n",
      "  [bt] (2) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x38303b) [0x79c410f8303b]\n",
      "  [bt] (3) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x3840ac) [0x79c410f840ac]\n",
      "  [bt] (4) /lib/x86_64-linux-gnu/libc.so.6(+0xa1ed3) [0x79c5272a1ed3]\n",
      "  [bt] (5) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x382f68) [0x79c410f82f68]\n",
      "  [bt] (6) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5cb55) [0x79c41165cb55]\n",
      "  [bt] (7) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5bacd) [0x79c41165bacd]\n",
      "  [bt] (8) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xd940b6) [0x79c4119940b6]\n",
      "2025-12-12 06:47:44,699\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=15107, ip=192.168.69.68)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 191, in _fit\n",
      "    self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1683, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n",
      "    if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/callback.py\", line 264, in after_iteration\n",
      "    score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 2352, in eval_set\n",
      "    _check_call(\n",
      "  File \"/home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/core.py\", line 310, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [06:47:38] /workspace/src/common/common.cu:16: /workspace/src/common/cuda_dr_utils.cc: 30: cudaErrorNoDevice: no CUDA-capable device is detected\n",
      "Stack trace:\n",
      "  [bt] (0) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x2a6e7c) [0x7e2cd92a6e7c]\n",
      "  [bt] (1) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5b61e) [0x7e2cd9a5b61e]\n",
      "  [bt] (2) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x38303b) [0x7e2cd938303b]\n",
      "  [bt] (3) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x3840ac) [0x7e2cd93840ac]\n",
      "  [bt] (4) /lib/x86_64-linux-gnu/libc.so.6(+0xa1ed3) [0x7e2def4a1ed3]\n",
      "  [bt] (5) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0x382f68) [0x7e2cd9382f68]\n",
      "  [bt] (6) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5cb55) [0x7e2cd9a5cb55]\n",
      "  [bt] (7) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xa5bacd) [0x7e2cd9a5bacd]\n",
      "  [bt] (8) /home/doawgh/nprint/.venv/lib/python3.12/site-packages/xgboost/lib/libxgboost.so(+0xd940b6) [0x7e2cd9d940b6]\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.04%)\n",
      "\t0.1895\t = Validation score   (balanced_accuracy)\n",
      "\t10.37s\t = Training   runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 48.33s of the 48.32s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 35.24% memory usage per fold, 70.48%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=8, gpus=0, memory=35.24%)\n",
      "\t0.3413\t = Validation score   (balanced_accuracy)\n",
      "\t40.2s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 4.73s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 0.421, 'CatBoost_BAG_L1': 0.316, 'RandomForestEntr_BAG_L1': 0.263}\n",
      "\t0.4164\t = Validation score   (balanced_accuracy)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 417.84s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: -37.9 rows/s (20 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "Metric balanced_accuracy is not supported by this model - using log_loss instead\n",
      "\tStopping at the best epoch learned earlier - 1.\n",
      "\t0.36s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\t1.89s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\t0.94s\t = Training   runtime\n",
      "Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t1.48s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t1.04s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\t52.77s\t = Training   runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t1.09s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t1.12s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
      "/home/doawgh/nprint/.venv/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t2.82s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
      "\t9.77s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 0.421, 'CatBoost_BAG_L1': 0.316, 'RandomForestEntr_BAG_L1': 0.263}\n",
      "\t0.13s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 68.71s ... Best model: \"WeightedEnsemble_L2_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/doawgh/nprint/AutogluonModels/ag-20251212_123846\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'balanced_accuracy': np.float64(0.43589743589743607), 'accuracy': 0.45, 'mcc': 0.402978130659247}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from autogluon.tabular import TabularPredictor\n",
    "data = X_reduced_df.copy()\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data['Label'])\n",
    "label_column = 'Label'\n",
    "predictor = TabularPredictor(label=label_column, eval_metric='balanced_accuracy').fit(\n",
    "    train_data,\n",
    "    # goes up to higher qualities, like best quality\n",
    "    presets='good_quality',\n",
    "    # increase training time if you want higher accuracy\n",
    "    # unfortunately my computer is garbage so this took me really long \n",
    "    time_limit=600            \n",
    ")\n",
    "performance = predictor.evaluate(test_data)\n",
    "print(performance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
